{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from game_tab import TicTacToe\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(board, board_size=3, players={0: '?', 1: 'X', 2: 'O'}):\n",
    "    '''\n",
    "    For fast visualization during debugging.\n",
    "    Input:\n",
    "    board <str>: Expected to be the board hash. Eg: '[1, 2, 1, 2, 1, 2, 1, 2, 1]'.\n",
    "    board_size <int>: Default to be 3. If you are using a board with different size, remember to specify this.\n",
    "    players <dict>: A dictionary for mapping numbers to symbols for better readability of the game board.\n",
    "    '''\n",
    "    for i, c in enumerate([int(x) for x in board[1:-1].split(',')]):\n",
    "        if i % board_size == 0:\n",
    "            print()\n",
    "        print(players[c], end='\\t')\n",
    "    print()\n",
    "\n",
    "\n",
    "class Environment(TicTacToe):\n",
    "    def __init__(self, first_player=1, debug=False, **kwargs):\n",
    "        '''\n",
    "        Initialize the environment.\n",
    "        Input:\n",
    "        first_player <int>: Representing the starting player.\n",
    "        debug <bool>: Default to False. If it was set to True, it will display debugging information.\n",
    "        kwargs <dict>: A dictionary for holding keyword arguments being passed.\n",
    "        '''\n",
    "        self.first_player = first_player\n",
    "        self.debug = debug\n",
    "        self.kwargs = kwargs\n",
    "        if self.kwargs == {}:\n",
    "            super().__init__()             # Initializes the game\n",
    "        else:\n",
    "            super().__init__(self.kwargs)  # Initializes the game with keyword arguments\n",
    "\n",
    "    @staticmethod\n",
    "    def getHash(board):\n",
    "        '''\n",
    "        Get hash of the current board.\n",
    "        Input:\n",
    "        board <list>: A board which is of list type. Eg: [1, 2, 1, 2, 1, 2, 1, 2, 1].\n",
    "        \n",
    "        Return:\n",
    "        <str>: The hash of the current state of the game.\n",
    "        '''\n",
    "        return str(board.tolist())\n",
    "\n",
    "    def reset(self, first_player=None):\n",
    "        '''\n",
    "        For resetting the environment after the game has finished. Keyword arguments being passed during initialization of the environment will be passed again for re-initialization.\n",
    "        Input:\n",
    "        first_player <int>: Representing the starting player.\n",
    "        \n",
    "        Return:\n",
    "        <str>: Hash of a board with only empty spaces.\n",
    "        '''\n",
    "        if self.kwargs == {}:\n",
    "            super().__init__()             # Initializes the game\n",
    "        else:\n",
    "            super().__init__(self.kwargs)  # Initializes the game with keyword arguments\n",
    "        if first_player is not None:\n",
    "            self.player = first_player\n",
    "        return self.getHash(self.board)\n",
    "\n",
    "    def render(self):\n",
    "        ''' Renders the current environment. '''\n",
    "        self.visualize()\n",
    "\n",
    "    def getAvailableMoves(self):\n",
    "        ''' Get available moves of the current board. '''\n",
    "        return np.where(self.board == self.EMPTY)[0]\n",
    "\n",
    "    def action_space(self):\n",
    "        ''' The action space of the current environment. A fancy wrap for consistency with Gym API. '''\n",
    "        return self.getAvailableMoves()\n",
    "\n",
    "    @staticmethod\n",
    "    def getReward(state, maximized_player):\n",
    "        '''\n",
    "        Get the reward given the game state and the player to be maximized for his reward.\n",
    "        Input:\n",
    "        state <int>: The game state of the game. Possible values: -1: Tie game; 0: The game hasn't ended yet; 1: Player 1 won; 2: Player 2 won, ...\n",
    "        maximized_player <int>: The player to be maximized for his reward.\n",
    "\n",
    "        Return:\n",
    "        <int>: Representing the reward in different situations.\n",
    "        '''\n",
    "        if state == -1:                  # No one wins\n",
    "            return 0\n",
    "        elif state == 0:                 # The game hasn't ended yet\n",
    "            return 0\n",
    "        elif state == maximized_player:  # The player which is to be maximized won\n",
    "            return 1\n",
    "        else:                            # Other players won\n",
    "            return -1\n",
    "\n",
    "    def step(self, action, player, maximized_player):\n",
    "        '''\n",
    "        Input:\n",
    "        action <int>: The position of the move to be placed. It is 0-based, with range [0.. <board_size>].\n",
    "        player <int>: The integer that represents the player. Eg 1 for player 1, 2 for player 2, ...\n",
    "        maximized_player <int>: The player to be maximized for his reward.\n",
    "\n",
    "        Return:\n",
    "        observation <str>: The hash of the board before the move\n",
    "        reward <int>: The reward due to the move\n",
    "        game_state <int>: The game state of the game. Possible values: -1: Tie game; 0: The game hasn't ended yet; 1: Player 1 won; 2: Player 2 won, ...\n",
    "        info <func>: For explaining the current situation of the game.\n",
    "        '''\n",
    "        if self.debug:\n",
    "            print('Player:', player, 'Action:', action, 'Action space:', self.action_space())\n",
    "        game_state = self.placeMove(action)\n",
    "        observation = self.getHash(self.board)\n",
    "        reward = self.getReward(game_state, maximized_player)\n",
    "        if self.debug:\n",
    "            print('Maximized Player:', maximized_player, 'Game State:', game_state, 'Reward:', reward, 'Observation:', observation)\n",
    "            self.visualize()\n",
    "            self.explain()\n",
    "            input()\n",
    "        info = self.explain\n",
    "        return observation, reward, game_state, info\n",
    "\n",
    "    def close(self):\n",
    "        ''' For shutting down the environment. A fancy wrap for consistency with Gym API. '''\n",
    "        self.game = None\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "        pass\n",
    "\n",
    "    def getAction(self, observation, action_space):\n",
    "        '''\n",
    "        To get action given the observation of the environment and the action space. As this is the most basic form of the agent, it will just return random actions.\n",
    "        Input:\n",
    "        observation <str>: The hash of the current state of the game.\n",
    "        action_space <list>: A list of integers indicating available moves of the current state of the game.\n",
    "        \n",
    "        Return:\n",
    "        action <int>: The move to be placed on the board.\n",
    "        '''\n",
    "        action = np.random.choice(action_space)\n",
    "        return action\n",
    "\n",
    "    def train(self, experience, f):\n",
    "        ''' To train the agent. As this is the most basic form of the agent, it could not be trained. '''\n",
    "        if self.debug:\n",
    "            print('The agent is unable to learn.')\n",
    "            input()\n",
    "        pass\n",
    "\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, discount_rate=0.9, learning_rate=1, epsilon=1, decay_rate=0.9999, debug=False):\n",
    "        '''\n",
    "        discount_rate <float>: To discount the reward if it takes more turns to get the reward. Eg if the reward is available after 2 turns, reward for the current move will be <reward> * discount_rate ** 2.\n",
    "        learning_rate <float>: To specify the speed of learning from moves given rewards. Since this is a deterministic game, learning_rate could be set to 1.\n",
    "        epsilon <float>: Representing the decay of trade-offs between exploration and exploitation.\n",
    "        debug <bool>: Default to False. If it was set to True, it will display debugging information.\n",
    "        '''\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.q_table = {}\n",
    "        self.debug = debug\n",
    "\n",
    "    @staticmethod\n",
    "    def select_argmax(dictionary):\n",
    "        '''\n",
    "        Returns the move that maximizes the reward. If there are several moves with the maximum reward, it will randomly choose one in these moves.\n",
    "        Input:\n",
    "        dictionary <dict>: A dictionary that contains available moves and their corresponding rewards.\n",
    "        \n",
    "        Return:\n",
    "        <int>: The action to be taken by the QAgent.\n",
    "        '''\n",
    "        if len(dictionary) == 0:\n",
    "            raise ValueError('The length of the dictionary should not be zero!')\n",
    "        max_q_value = max(dictionary.values())\n",
    "        choices = []\n",
    "        for key in dictionary:\n",
    "            if dictionary[key] == max_q_value:\n",
    "                choices.append(key)\n",
    "        return np.random.choice(choices)\n",
    "\n",
    "    @staticmethod\n",
    "    def select_argmin(dictionary):\n",
    "        '''\n",
    "        Returns the move that minimizes the reward. If there are several moves with the minimum reward, it will randomly choose one in these moves.\n",
    "        Input:\n",
    "        dictionary <dict>: A dictionary that contains available moves and their corresponding rewards.\n",
    "        \n",
    "        Return:\n",
    "        <int>: The exploitation (greedy) action to be taken by the QAgent.\n",
    "        '''\n",
    "        if len(dictionary) == 0:\n",
    "            raise ValueError('The length of the dictionary should not be zero!')\n",
    "        min_q_value = min(dictionary.values())\n",
    "        choices = []\n",
    "        for key in dictionary:\n",
    "            if dictionary[key] == min_q_value:\n",
    "                choices.append(key)\n",
    "        return np.random.choice(choices)\n",
    "\n",
    "    def getActionMax(self, observation, action_space):\n",
    "        '''\n",
    "        To get the action that maximizes the reward. It will be either an exploration or exploitation action, subject to the epsilon.\n",
    "        Input:\n",
    "        observation <str>: The hash of the current state of the game.\n",
    "        action_space <list>: A list of integers indicating available moves of the current state of the game.\n",
    "        \n",
    "        Return:\n",
    "        <int>: An action to be taken by the QAgent.\n",
    "        '''\n",
    "        action_random = super().getAction(observation, action_space)\n",
    "        q_value_dict = {}\n",
    "        for action in action_space:\n",
    "            q_value_dict[action] = self.q_table.get(observation, {}).get(action, 0)\n",
    "        action_greedy = self.select_argmax(q_value_dict)\n",
    "        if self.debug:\n",
    "            print('Action random:', action_random, 'Action greedy:', action_greedy)\n",
    "        return action_random if np.random.random() < self.epsilon else action_greedy\n",
    "\n",
    "    def getActionMin(self, observation, action_space):\n",
    "        '''\n",
    "        To get the action that minimizes the reward. It will be either an exploration or exploitation action, subject to the epsilon.\n",
    "        Input:\n",
    "        observation <str>: The hash of the current state of the game.\n",
    "        action_space <list>: A list of integers indicating available moves of the current state of the game.\n",
    "        \n",
    "        Return:\n",
    "        <int>: An action to be taken by the QAgent.\n",
    "        '''\n",
    "        action_random = super().getAction(observation, action_space)\n",
    "        q_value_dict = {}\n",
    "        for action in action_space:\n",
    "            q_value_dict[action] = self.q_table.get(observation, {}).get(action, 0)\n",
    "        action_greedy = self.select_argmin(q_value_dict)\n",
    "        if self.debug:\n",
    "            print('Action random:', action_random, 'action greedy:', action_greedy)\n",
    "        return action_random if np.random.random() < self.epsilon else action_greedy\n",
    "\n",
    "    def train(self, experience, f):\n",
    "        '''\n",
    "        To train the QAgent, given experience replay and a function representing whether to maximize or minimize the action.\n",
    "        Input:\n",
    "        experience <tuple>: The experiene replay of the game. It could be decomposed into five variables: observation, action, next_observation, reward, game_state.\n",
    "        f <func>: A function representing whether to maximize or minimize the action of the player.\n",
    "        \n",
    "        Important variables:\n",
    "        observation <str>: The hash of the board before the move\n",
    "        action <int>: The position of the move to be placed. It is 0-based, with range [0.. <board_size>].\n",
    "        next_observation <str>: The hash of the board after the move\n",
    "        reward <int>: The reward due to the move\n",
    "        game_state <int>: The game state of the game. Possible values: -1: Tie game; 0: The game hasn't ended yet; 1: Player 1 won; 2: Player 2 won, ...\n",
    "        current_state_q_value: The Q-value of the current state. It is retrieved from the QAgent's Q-table.\n",
    "        next_state_best_q_value: The predicted best Q-value of the next state. Since this is a minimax game, its objective is to minimize other players' rewards while maximizing self reward. It is retrieved from the QAgent's Q-table.\n",
    "        '''\n",
    "        observation, action, next_observation, reward, game_state = experience\n",
    "\n",
    "        if self.q_table.get(observation) is None:         # Create a new entry for the q_table with the newly observed state\n",
    "            self.q_table[observation] = {action: 0}       # Initialize the q_value of the action to 0\n",
    "\n",
    "        if self.q_table[observation].get(action) is None: # What if such observation exists, but such action does not exist?\n",
    "            self.q_table[observation][action] = 0         # Initialize the q_value of the action to 0\n",
    "\n",
    "        if self.debug:\n",
    "            print('observation:', observation)\n",
    "            print('action:', action)\n",
    "            print('self.q_table[observation]:', self.q_table[observation])\n",
    "            print('current_state_q_value:', self.q_table[observation][action])\n",
    "            print('next_state_best_q_value:', f(self.q_table.get(next_observation, {0: 0}).values()))\n",
    "\n",
    "        next_state_best_q_value = f(self.q_table.get(next_observation, {0: 0}).values())\n",
    "        current_state_q_value = self.q_table[observation][action]\n",
    "        if current_state_q_value != 1:\n",
    "            self.q_table[observation][action] = current_state_q_value + self.learning_rate * (reward + self.discount_rate * next_state_best_q_value - current_state_q_value)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f'self.q_table[observation][action]: {self.q_table[observation][action]} = {current_state_q_value} + {self.learning_rate} * ({reward} + {self.discount_rate} * {next_state_best_q_value} - {current_state_q_value})')\n",
    "            input()\n",
    "\n",
    "        if game_state:\n",
    "            self.epsilon *= self.decay_rate\n",
    "\n",
    "\n",
    "def collect_statistics(stat, result):\n",
    "    '''\n",
    "    stat <list>: a list that stores the result of every game play.\n",
    "    result <int>: the game state that indicates the result of the game.\n",
    "    '''\n",
    "    stat.append(result)\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Random Agent vs Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = Environment(debug=False)\n",
    "random_agent = Agent(debug=False)\n",
    "episode = 50000\n",
    "stat = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i_episode in range(episode):\n",
    "    observation = env.reset()\n",
    "    i = 0\n",
    "    for t in range(100):\n",
    "        player = env.getCurrentPlayer()\n",
    "        if player == 1:\n",
    "            action = random_agent.getAction(observation, env.getAvailableMoves())\n",
    "            next_observation, reward, game_state, info = env.step(action, player, 1)\n",
    "            random_agent.train(None, None)\n",
    "        elif player == 2:\n",
    "            action = random_agent.getAction(observation, env.getAvailableMoves())\n",
    "            next_observation, reward, game_state, info = env.step(action, player, 1)\n",
    "            random_agent.train(None, None)\n",
    "        observation = next_observation\n",
    "        if game_state != 0:\n",
    "            stat = collect_statistics(stat, env.checkGameEnds())\n",
    "            if (i_episode + 1) % 10000 == 0:\n",
    "                print(f'Episode {i_episode + 1}, {time.time() - start_time:.2f}s elapsed')\n",
    "                start_time = time.time()\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First decode to three columns\n",
    "summary = pd.get_dummies(stat)\n",
    "draw = summary[-1]\n",
    "agent_1 = summary[1]\n",
    "agent_2 = summary[2]\n",
    "\n",
    "# Then apply rolling(100).mean() to each of them\n",
    "# Finally plot them out\n",
    "interval = 100  # Take samples to calculate game outcomes per <interval>\n",
    "plt.plot(draw.rolling(interval).mean()[::interval], label='Draw')\n",
    "plt.plot(agent_1.rolling(interval).mean()[::interval], label='Agent 1 Win')\n",
    "plt.plot(agent_2.rolling(interval).mean()[::interval], label='Agent 2 Win')\n",
    "plt.legend()\n",
    "plt.title(f'{episode} Games Played by Random Agent vs Random Agent')\n",
    "plt.xlabel('Number of Games Played')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Random Agent vs Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we set epsilon to 0, to stop exploration and only retain expolitation.\n",
    "# Let it play 10000 games we check the result of the game.\n",
    "env.debug = False\n",
    "random_agent.debug = False\n",
    "episode = 10000\n",
    "test_stat = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i_episode in range(episode):\n",
    "    first_player = 1\n",
    "    observation = env.reset(first_player=first_player)\n",
    "    i = 0\n",
    "    for t in range(100):\n",
    "        player = env.getCurrentPlayer()\n",
    "        if player == 1:\n",
    "            action = random_agent.getAction(observation, env.getAvailableMoves())\n",
    "            next_observation, reward, game_state, info = env.step(action, player, 1)\n",
    "        elif player == 2:\n",
    "            action = random_agent.getAction(observation, env.getAvailableMoves())\n",
    "            next_observation, reward, game_state, info = env.step(action, player, 1)\n",
    "        observation = next_observation\n",
    "        if game_state != 0:\n",
    "            test_stat = collect_statistics(test_stat, env.checkGameEnds())\n",
    "            if (i_episode + 1) % 10000 == 0:\n",
    "                print(f'Episode {i_episode + 1}, {time.time() - start_time:.2f}s elapsed')\n",
    "                start_time = time.time()\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "# First decode to three columns\n",
    "test_summary = pd.get_dummies(test_stat)\n",
    "draw = test_summary.get(-1, pd.Series(index=test_stat, name=-1))\n",
    "agent_1 = test_summary.get(1, pd.Series([0] * len(test_stat), name=1))\n",
    "agent_2 = test_summary.get(2, pd.Series([0] * len(test_stat), name=2))\n",
    "\n",
    "# Then apply rolling(100).mean() to each of them\n",
    "# Finally plot them out\n",
    "interval = 100  # Take samples to calculate game outcomes per <interval>\n",
    "plt.plot(draw.rolling(interval).mean()[::interval], label='Draw')\n",
    "plt.plot(agent_1.rolling(interval).mean()[::interval], label='Agent 1 Win')\n",
    "plt.plot(agent_2.rolling(interval).mean()[::interval], label='Agent 2 Win')\n",
    "plt.legend()\n",
    "plt.title('Games Played by Random Agent vs Random Agent')\n",
    "plt.xlabel('Number of Games Played')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()\n",
    "\n",
    "print(f'QAgent vs QAgent, for the past {episode} games:')\n",
    "print(f'Draw Rate        : {draw.mean()} ({pd.Series(test_stat).value_counts().get(-1, 0)} Games)')\n",
    "print(f'Agent 1 Win Rate : {agent_1.mean()} ({pd.Series(test_stat).value_counts().get(1, 0)} Games)')\n",
    "print(f'Agent 2 Win Rate : {agent_2.mean()} ({pd.Series(test_stat).value_counts().get(2, 0)} Games)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the random baseline of the game. A good agent should surpass the draw rate given by two random agents. Therefore, we should expect to have a higher draw rate or win rate after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training QAgent vs QAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "episode = 100000\n",
    "env = Environment(debug=False)\n",
    "q_agent = QAgent(debug=False, decay_rate=0.001 ** (1 / episode))\n",
    "stat = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i_episode in range(episode):\n",
    "    observation = env.reset()\n",
    "    i = 0\n",
    "    for t in range(100):\n",
    "        player = env.getCurrentPlayer()\n",
    "        if player == 1:\n",
    "            action = q_agent.getActionMax(observation, env.getAvailableMoves())\n",
    "            next_observation, reward, game_state, info = env.step(action, player, 1)\n",
    "            q_agent.train((observation, action, next_observation, reward, game_state), min)\n",
    "        elif player == 2:\n",
    "            action = q_agent.getActionMin(observation, env.getAvailableMoves())\n",
    "            next_observation, reward, game_state, info = env.step(action, player, 1)\n",
    "            q_agent.train((observation, action, next_observation, reward, game_state), max)\n",
    "        observation = next_observation\n",
    "        if game_state != 0:\n",
    "            stat = collect_statistics(stat, env.checkGameEnds())\n",
    "            if (i_episode + 1) % 10000 == 0:\n",
    "                print(f'Episode {i_episode + 1}, {time.time() - start_time:.2f}s elapsed')\n",
    "                start_time = time.time()\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First decode to three columns\n",
    "summary = pd.get_dummies(stat)\n",
    "draw = summary[-1]\n",
    "agent_1 = summary[1]\n",
    "agent_2 = summary[2]\n",
    "\n",
    "# Then apply rolling(100).mean() to each of them\n",
    "# Finally plot them out\n",
    "interval = 100  # Take samples to calculate game outcomes per <interval>\n",
    "plt.plot(draw.rolling(interval).mean()[::interval], label='Draw')\n",
    "plt.plot(agent_1.rolling(interval).mean()[::interval], label='Agent 1 Win')\n",
    "plt.plot(agent_2.rolling(interval).mean()[::interval], label='Agent 2 Win')\n",
    "plt.legend()\n",
    "plt.title(f'{episode} Games Played by QAgent vs QAgent')\n",
    "plt.xlabel('Number of Games Played')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing QAgent vs QAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we set epsilon to 0, to stop exploration and only retain expolitation.\n",
    "# Let it play 10000 games we check the result of the game.\n",
    "\n",
    "q_agent.epsilon = 0\n",
    "episode = 50000\n",
    "test_stat = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i_episode in range(episode):\n",
    "    observation = env.reset()\n",
    "    i = 0\n",
    "    for t in range(100):\n",
    "        player = env.getCurrentPlayer()\n",
    "        if player == first_player:\n",
    "            action = q_agent.getActionMax(observation, env.getAvailableMoves())\n",
    "            next_observation, reward, game_state, info = env.step(action, player, 1)\n",
    "        else:\n",
    "            action = q_agent.getActionMin(observation, env.getAvailableMoves())\n",
    "            next_observation, reward, game_state, info = env.step(action, player, 1)\n",
    "        observation = next_observation\n",
    "        if game_state != 0:\n",
    "            test_stat = collect_statistics(test_stat, env.checkGameEnds())\n",
    "            if (i_episode + 1) % 10000 == 0:\n",
    "                print(f'Episode {i_episode + 1}, {time.time() - start_time:.2f}s elapsed')\n",
    "                start_time = time.time()\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "# First decode gameplay results to three columns\n",
    "test_summary = pd.get_dummies(test_stat)\n",
    "draw = test_summary.get(-1, pd.Series(index=test_stat, name=-1))\n",
    "agent_1 = test_summary.get(1, pd.Series([0] * len(test_stat), name=1))\n",
    "agent_2 = test_summary.get(2, pd.Series([0] * len(test_stat), name=2))\n",
    "\n",
    "# Then apply rolling(100).mean() to each of them to calculate average draw or win rate per 100 intervals\n",
    "# Finally plot them out\n",
    "interval = 100  # Take samples to calculate game outcomes per <interval>\n",
    "plt.plot(draw.rolling(interval).mean()[::interval], label='Draw')\n",
    "plt.plot(agent_1.rolling(interval).mean()[::interval], label='Agent 1 Win')\n",
    "plt.plot(agent_2.rolling(interval).mean()[::interval], label='Agent 2 Win')\n",
    "plt.legend()\n",
    "plt.title('Games Played by QAgent vs QAgent')\n",
    "plt.xlabel('Number of Games Played')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()\n",
    "\n",
    "print(f'QAgent vs QAgent, for the past {episode} games:')\n",
    "print(f'Draw Rate        : {draw.mean()} ({pd.Series(test_stat).value_counts().get(-1, 0)} Games)')\n",
    "print(f'Agent 1 Win Rate : {agent_1.mean()} ({pd.Series(test_stat).value_counts().get(1, 0)} Games)')\n",
    "print(f'Agent 2 Win Rate : {agent_2.mean()} ({pd.Series(test_stat).value_counts().get(2, 0)} Games)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q_agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "f = open('table', 'wb')\n",
    "\n",
    "pk.dump(q_agent.q_table, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the agent works perfectly in different situations, we should write test cases and test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "board = '[1, 1, 0, 2, 2, 0, 1, 2, 1]'\n",
    "visualize(board)\n",
    "print(q_agent.q_table[board])\n",
    "if q_agent.q_table[board][2] > q_agent.q_table[board][5]:\n",
    "    print('OK')\n",
    "else:\n",
    "    print('Failed, position 2 should have greater q value than position 5.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "board = '[1, 0, 0, 2, 2, 1, 1, 2, 1]'\n",
    "visualize(board)\n",
    "print(q_agent.q_table[board])\n",
    "\n",
    "if q_agent.q_table[board][1] < q_agent.q_table[board][2]:\n",
    "    print('OK')\n",
    "else:\n",
    "    print('Failed, position 1 should have smaller q value than position 2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "board = '[1, 0, 0, 2, 2, 0, 1, 2, 1]'\n",
    "visualize(board)\n",
    "print(q_agent.q_table[board])\n",
    "if q_agent.q_table[board][1] == q_agent.q_table[board][5]:\n",
    "    print('OK')\n",
    "else:\n",
    "    print('Failed, position 1 should have the same q value with position 5.')\n",
    "\n",
    "if q_agent.q_table[board][1] != q_agent.q_table[board][2] and q_agent.q_table[board][5] != q_agent.q_table[board][2]:\n",
    "    print('OK')\n",
    "else:\n",
    "    print('Failed, position 1 and position 5 should have more negative q value when compared with position 2.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above case, we found that when the computer thinks that it will lose anyway, it will just give up and treat these positions indifferently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "board = '[1, 0, 0, 0, 1, 0, 2, 0, 0]'\n",
    "visualize(board)\n",
    "print(q_agent.q_table[board])\n",
    "if q_agent.q_table[board][8] == min(q_agent.q_table[board].values()):\n",
    "    print('OK')\n",
    "else:\n",
    "    print('Failed, position 8 should have the smallest q value.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "board = '[2, 0, 2, 0, 1, 0, 1, 1, 0]'\n",
    "visualize(board)\n",
    "print(q_agent.q_table[board])\n",
    "if q_agent.q_table[board][1] == min(q_agent.q_table[board].values()):\n",
    "    print('OK')\n",
    "else:\n",
    "    print('Failed, position 1 should have the smallest q value.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "board = '[0, 0, 2, 1, 0, 2, 0, 1, 0]'\n",
    "visualize(board)\n",
    "print(q_agent.q_table[board])\n",
    "if q_agent.q_table[board][8] == max(q_agent.q_table[board].values()):\n",
    "    print('OK')\n",
    "else:\n",
    "    print('Failed, position 8 should have either the largest q value.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about QAgent vs Random Agent? Does QAgent always superior to Random Agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing QAgent vs Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we set epsilon to 0, to stop exploration and only retain expolitation.\n",
    "# Let it play 10000 games we check the result of the game.\n",
    "\n",
    "q_agent.epsilon = 0\n",
    "episode = 10000\n",
    "test_stat = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i_episode in range(episode):\n",
    "    observation = env.reset()\n",
    "    history = []\n",
    "    history.append(observation)\n",
    "    i = 0\n",
    "    for t in range(100):\n",
    "        player = env.getCurrentPlayer()\n",
    "        if player == 1:\n",
    "            if first_player == 1:\n",
    "                action = q_agent.getActionMax(observation, env.getAvailableMoves())\n",
    "            else:\n",
    "                action = q_agent.getActionMin(observation, env.getAvailableMoves())\n",
    "            next_observation, reward, game_state, info = env.step(action, player, 1)\n",
    "        elif player == 2:\n",
    "            action = random_agent.getAction(observation, env.getAvailableMoves())\n",
    "            next_observation, reward, game_state, info = env.step(action, player, 1)\n",
    "        observation = next_observation\n",
    "        history.append(observation)\n",
    "        if game_state != 0:\n",
    "            test_stat = collect_statistics(test_stat, env.checkGameEnds())\n",
    "            if (i_episode + 1) % 10000 == 0:\n",
    "                print(f'Episode {i_episode + 1}, {time.time() - start_time:.2f}s elapsed')\n",
    "                start_time = time.time()\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "# First decode to three columns\n",
    "test_summary = pd.get_dummies(test_stat)\n",
    "draw = test_summary.get(-1, pd.Series(index=test_stat, name=-1))\n",
    "agent_1 = test_summary.get(1, pd.Series([0] * len(test_stat), name=1))\n",
    "agent_2 = test_summary.get(2, pd.Series([0] * len(test_stat), name=2))\n",
    "\n",
    "# Then apply rolling(100).mean() to each of them\n",
    "# Finally plot them out\n",
    "interval = 100  # Take samples to calculate game outcomes per <interval>\n",
    "plt.plot(draw.rolling(interval).mean()[::interval], label='Draw')\n",
    "plt.plot(agent_1.rolling(interval).mean()[::interval], label='Agent 1 Win')\n",
    "plt.plot(agent_2.rolling(interval).mean()[::interval], label='Agent 2 Win')\n",
    "plt.legend()\n",
    "plt.title('Games played by QAgent vs Random Agent')\n",
    "plt.xlabel('Number of games played')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()\n",
    "\n",
    "print(f'QAgent vs QAgent, for the past {episode} games:')\n",
    "print(f'Draw Rate        : {draw.mean()} ({pd.Series(test_stat).value_counts().get(-1, 0)} Games)')\n",
    "print(f'Agent 1 Win Rate : {agent_1.mean()} ({pd.Series(test_stat).value_counts().get(1, 0)} Games)')\n",
    "print(f'Agent 2 Win Rate : {agent_2.mean()} ({pd.Series(test_stat).value_counts().get(2, 0)} Games)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play some games with our QAgent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = input('Would you like to start first? Enter anything to start as the first player, or just press <Enter> to become the second player: ')\n",
    "if answer != '':\n",
    "    HUMAN_PLAYER = 1\n",
    "    COMPUTER_PLAYER = 2\n",
    "    first_player = HUMAN_PLAYER\n",
    "else:\n",
    "    COMPUTER_PLAYER = 1\n",
    "    HUMAN_PLAYER = 2\n",
    "    first_player = COMPUTER_PLAYER\n",
    "\n",
    "observation = env.reset(first_player=first_player)\n",
    "q_agent.epsilon = 0\n",
    "env.render()\n",
    "i = 0\n",
    "for t in range(100):\n",
    "    player = env.getCurrentPlayer()\n",
    "    if player == HUMAN_PLAYER:\n",
    "        action = int(input(f'Human\\'s move (0 to {env.board_size ** 2 - 1}): '))\n",
    "    elif player == COMPUTER_PLAYER:\n",
    "        if first_player == COMPUTER_PLAYER:\n",
    "            action = q_agent.getActionMax(observation, env.getAvailableMoves())\n",
    "        else:\n",
    "            action = q_agent.getActionMin(observation, env.getAvailableMoves())\n",
    "        print(f'QAgent\\'s move (0 to {env.board_size ** 2 - 1}): {action}')\n",
    "    next_observation, reward, done, info = env.step(action, player, True)\n",
    "    observation = next_observation\n",
    "    reward += reward\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "#         sleep(2)\n",
    "#     clear_output(wait=True)\n",
    "env.explain()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Directions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}